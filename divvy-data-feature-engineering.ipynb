{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7588c7a8",
   "metadata": {
    "papermill": {
     "duration": 0.049244,
     "end_time": "2022-04-30T23:15:46.180545",
     "exception": false,
     "start_time": "2022-04-30T23:15:46.131301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Divvy Data (Feature) Engineering \n",
    "### Summary:\n",
    "This module transforms travel history data into time sereies data of the station dock capacity\n",
    "### Challenge & solutions\n",
    "1. Data formats are not consistent -> Manually transform the data \n",
    "2. There are ',' in csv file -> Manually transform the data \n",
    "3. Data size is relatively big -> Use big data platforms to perform the aggregation \n",
    "4. Station_ID is inconsistent across the years -> Use Station_Name as Primary key instead \n",
    "### Methodology\n",
    "1. Clean the data by aggregate into a single table \n",
    "2. Extract Day+Hour+10Min as Time_ID, group by Time_ID, Station_Name, aggregate by count\n",
    "3. Create placeholding dataframe df_main \n",
    "4. Join each station into the placeholding dataframe to create a pivoted timeseries table, where each row is a time snapshot, each column is the time series of each station \n",
    "5. Merge In & Out to form Capacity Change\n",
    "6. Finally, will compare the artificial delta agianst the actual station data and intepret the difference\n",
    "### Data\n",
    "- Source: https://www.divvybikes.com/system-data\n",
    "- Description: This dataset includes individual Divvy bike sharing trips, including the origin, destination, timestamps, and rider type for each trip.\n",
    "\n",
    "### Author: \n",
    "`Ryan Liao @2022/04/30 M.Sc Data Analytics Uchicago`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a870a8f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T23:15:46.277926Z",
     "iopub.status.busy": "2022-04-30T23:15:46.277538Z",
     "iopub.status.idle": "2022-04-30T23:15:47.687922Z",
     "shell.execute_reply": "2022-04-30T23:15:47.686399Z"
    },
    "papermill": {
     "duration": 1.462701,
     "end_time": "2022-04-30T23:15:47.691116",
     "exception": false,
     "start_time": "2022-04-30T23:15:46.228415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a1708",
   "metadata": {
    "papermill": {
     "duration": 0.047702,
     "end_time": "2022-04-30T23:15:47.787842",
     "exception": false,
     "start_time": "2022-04-30T23:15:47.740140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### 1. Clean the data by aggregate into a single table \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0f22edc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T23:15:47.885291Z",
     "iopub.status.busy": "2022-04-30T23:15:47.885010Z",
     "iopub.status.idle": "2022-04-30T23:15:47.988358Z",
     "shell.execute_reply": "2022-04-30T23:15:47.987389Z"
    },
    "papermill": {
     "duration": 0.153984,
     "end_time": "2022-04-30T23:15:47.990191",
     "exception": true,
     "start_time": "2022-04-30T23:15:47.836207",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\Data\\\\divvy\\\\TimeS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/1649414471.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'E:\\\\Data\\\\divvy\\\\TimeS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\Data\\\\divvy\\\\TimeS'"
     ]
    }
   ],
   "source": [
    "os.listdir('E:\\\\Data\\\\divvy\\\\TimeS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50c1f3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.listdir('E:\\\\Data\\\\divvy\\\\Travel_Hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187af48",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Uniform the column names\n",
    "DATABASE = {}\n",
    "#unit_col = DATABASE['2017_Q1'].columns <- This depreciates\n",
    "for path in os.listdir('E:\\\\Data\\\\divvy\\\\Travel_Hist'):\n",
    "    file = \"E:\\\\Data\\\\divvy\\\\Travel_Hist\\\\\" + path \n",
    "    if '.csv' in file:\n",
    "        name = path.split('.')[0][-7:]\n",
    "        df = pd.read_csv(file)\n",
    "        df = df.rename({i:j for i,j in zip(df.columns,unit_col)},axis=1)\n",
    "        df.start_time = df.start_time.apply(\n",
    "            lambda x: datetime.strptime(x, \"%m/%d/%Y %H:%M:%S\")\n",
    "        )\n",
    "        df.end_time = df.end_time.apply(\n",
    "                lambda x: datetime.strptime(x, \"%m/%d/%Y %H:%M:%S\")\n",
    "            )\n",
    "        df.to_csv(file,index=False)\n",
    "        print(name)\n",
    "        #DATABASE[name] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9ac27",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load into a single dataframe\n",
    "DATABASE = {} \n",
    "for path in os.listdir('E:\\\\Data\\\\divvy\\\\Travel_Hist'):\n",
    "    file = \"E:\\\\Data\\\\divvy\\\\Travel_Hist\\\\\" + path \n",
    "    if '.csv' in file:\n",
    "        name = path.split('.')[0][-7:]\n",
    "        df = pd.read_csv(file)\n",
    "        DATABASE[name] = pd.read_csv(file)\n",
    "        #print(name)\n",
    "        #print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b433d55f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "for db_name in DATABASE:\n",
    "    df = DATABASE[db_name]\n",
    "    df_all = pd.concat([df_all,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c940ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a635b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Examine column types to identify areas that requiring transformation\n",
    "# start_time,end_time      -> Need to be mapped into datetime objects for easier manipulation\n",
    "# tripduration             -> Supposed to be float but instead containing werid formatted string, such as \"1,102\"\n",
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d447b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.start_time = df_all.start_time.apply(\n",
    "    lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    ")\n",
    "df_all.end_time = df_all.end_time.apply(\n",
    "        lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86f8e3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.start_time.head().apply(lambda x:str(x)[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4dfa00",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.tripduration = df_all.tripduration.apply(lambda x:float(str(x).replace(',','')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea5e45",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2. Extract Day+Hour+10Min as Time_ID, group by Time_ID, Station_Name, aggregate by count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14564a6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH10M_start = df_all.start_time.apply(lambda x:str(x)[:-4])\n",
    "DH10M_end = df_all.end_time.apply(lambda x:str(x)[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ab971",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all['DH10M_start'] = DH10M_start\n",
    "df_all['DH10M_end'] = DH10M_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c6a725",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Already Done\n",
    "ts_OUT =  pd.DataFrame(df_all.groupby(by=['DH10M_start','from_station_name']).trip_id.count())\n",
    "ts_IN =   pd.DataFrame(df_all.groupby(by=['DH10M_end','to_station_name']).trip_id.count())\n",
    "ts_IN = ts_IN.rename({'trip_id':'count_enter'},axis=1)\n",
    "ts_OUT = ts_OUT.rename({'trip_id':'count_leave'},axis=1)\n",
    "ts_OUT.DH10M_start = ts_OUT.DH10M_start.apply(lambda x: datetime.strptime(x+'0:00', \"%Y-%m-%d %H:%M:%S\"))\n",
    "ts_OUT = ts_OUT.set_index('DH10M_start',drop=True)\n",
    "ts_IN.DH10M_end = ts_IN.DH10M_end.apply(lambda x: datetime.strptime(x+'0:00', \"%Y-%m-%d %H:%M:%S\"))\n",
    "ts_IN = ts_IN.set_index('DH10M_end',drop=True)\n",
    "ts_IN.to_csv('E:\\\\Data\\\\divvy\\\\ts_IN.csv')\n",
    "ts_OUT.to_csv('E:\\\\Data\\\\divvy\\\\ts_OUT.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a9919",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts_IN = pd.read_csv('E:\\\\Data\\\\divvy\\\\ts_IN.csv')\n",
    "ts_OUT = pd.read_csv('E:\\\\Data\\\\divvy\\\\ts_OUT.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999dc3e6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3. Create placeholding dataframe df_main \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c1f60",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TS_gen(time_start,time_end,time_unit = timedelta(minutes=10)):\n",
    "    \"\"\"\n",
    "    __summary__\n",
    "    create a dataframe rangeing from time_start -> time_end\n",
    "    \"\"\"\n",
    "    out_dict = {'time_stamp':[]}\n",
    "    current_time = time_start\n",
    "    while current_time < time_end:\n",
    "        out_dict['time_stamp'].append(current_time)\n",
    "        current_time += time_unit\n",
    "    out = pd.DataFrame(out_dict)\n",
    "    out.time_stamp = out.time_stamp.apply(lambda x: str(x))\n",
    "    return out.set_index('time_stamp',drop=True)\n",
    "    \n",
    "def FillSpace(df,tablename,time_start,time_end,station_names,\n",
    "              station_col_name,count_name,time_stamp_name,file_path):\n",
    "    \"\"\"\n",
    "    __summary__\n",
    "    Create a pivoted time series dataframe\n",
    "    \"\"\"\n",
    "    ## Depreciated in newer version\n",
    "    # df['time']=df[f'{tablename}.{prefix}_at_time'].apply(lambda x: f' {x}0:00')\n",
    "    # df['start_timestamp'] = df[f'{tablename}.{prefix}_at_day'] + df['time']\n",
    "    # df['start_timestamp'] = df['start_timestamp'].apply(\n",
    "    #             lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "    #         )\n",
    "    print('data prepared')\n",
    "    ########## Joining the tables ###########\n",
    "    df_in_main = TS_gen(time_start,time_end)\n",
    "    print('df_main generated,joining now...')\n",
    "    count = 0 \n",
    "    total = len(station_names)\n",
    "    for station_name in tqdm(station_names):\n",
    "        count+=1\n",
    "        print(f'{station_name}|{count}/{total}')\n",
    "        df_sub_set = df[df[f'{station_col_name}'] == station_name]\n",
    "        df_sub_set = df_sub_set.set_index(time_stamp_name,drop=True)\n",
    "        df_in_main = df_in_main.join(df_sub_set[count_name]).fillna(0).rename({count_name:station_name},axis=1)\n",
    "    #Finally\n",
    "    df_in_main.to_csv(f'{file_path}\\\\{tablename}_ts_pivot.csv')\n",
    "    return df_in_main\n",
    "##########\n",
    "time_start = datetime(year =2017,month=1,day=1,hour=0,minute=20)\n",
    "time_end = datetime(year =2019,month=9,day=30,hour=23,minute=50)\n",
    "station_names = set(ts_IN.to_station_name.unique()).union(set(ts_OUT.from_station_name.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c990ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_in = FillSpace(ts_IN,\"TS_in_1719\",time_start,time_end,station_names,\n",
    "                     'to_station_name','count_enter','DH10M_end','E:\\\\Data\\\\divvy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18658690",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_out = FillSpace(ts_OUT,\"TS_OUT_1719\",time_start,time_end,station_names,\n",
    "          'from_station_name','count_leave','DH10M_start','E:\\\\Data\\\\divvy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f16445",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_out = pd.read_csv('E:\\\\Data\\\\divvy\\\\TS_OUT_1719_ts_pivot.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6dd80",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4. Join each station into the placeholding dataframe to create a pivoted timeseries table, where each row is a time snapshot, each column is the time series of each station "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e37f11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5. Merge In & Out to form Capacity Change; Visualize time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55228630",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_out['Damen Ave & Pierce Ave'].plot()\n",
    "df_ts_in['Damen Ave & Pierce Ave'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72250047",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_balance = df_ts_in - df_ts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045e4c0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_balance['Damen Ave & Pierce Ave'].plot(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c69c972",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_balance['Damen Ave & Pierce Ave'][:1000].plot(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc69270",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_balance.to_csv('E:\\\\Data\\\\divvy\\\\df_ts_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddc8c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This plots aggregated delta (assuming no staff manuvouring)\n",
    "hour_row = 6 \n",
    "day_row = 24 * hour_row\n",
    "station_name = df_ts_in.columns[0:5]#'Lake Park Ave & 47th St'\n",
    "df_ts_balance[station_name][:60*day_row].cumsum().plot(figsize=(16,9))\n",
    "plt.title(f'{station_name} dock delta aggregated')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')\n",
    "# df_ts_in[station_name].cumsum().plot(figsize=(16,9))\n",
    "# df_ts_out[station_name].cumsum().plot(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87523ba3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This plots rolling cumsum, resetting each day. \n",
    "df_ts_balance = df_ts_balance.reset_index()\n",
    "df_ts_balance.insert( 0, 'date' ,df_ts_balance.time_stamp.apply(lambda x:x.split(' ')[0]))\n",
    "df_ts_balance = df_ts_balance.set_index('time_stamp',drop=True)\n",
    "df_ts_balance_CS = df_ts_balance.groupby('date').cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757e304",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plotting\n",
    "hour_row = 6 \n",
    "day_row = 24 * hour_row\n",
    "station_name = df_ts_in.columns[0:5]#'Lake Park Ave & 47th St'\n",
    "df_ts_balance_CS[station_name][:30*day_row].plot(figsize=(16,9))\n",
    "plt.title(f'{station_name}  dock delta daily reset')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29172524",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Compare against actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662518c2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "station_name = 'California Ave & Milwaukee Ave'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0fd3f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_balance[station_name][:60*day_row].cumsum().plot(figsize=(16,9))\n",
    "plt.title(f'{station_name} dock delta aggregated')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870bce72",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_balance_CS[station_name][:30*day_row].plot(figsize=(16,9))\n",
    "plt.title(f'{station_name}  dock delta daily reset')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a347da2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dockts_real = pd.read_csv('E:\\\\Data\\\\divvy\\\\TimeS\\\\DivvyData_dock_hist1.csv')\n",
    "df_dockts_real.Timestamp = df_dockts_real.Timestamp.apply(\n",
    "            lambda x: str(datetime.strptime(x, \"%m/%d/%Y %H:%M:%S %p\"))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be29f077",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dockts_real = df_dockts_real.rename({'Total Docks':'Total_Docks'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4fdee",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Num_days = 7\n",
    "start = datetime.strptime('2017-01-01','%Y-%m-%d')\n",
    "end = start + timedelta(days=Num_days)\n",
    "df_dockts_real_subset = df_dockts_real.loc[(df_dockts_real['Timestamp']>=str(start)) & (df_dockts_real['Timestamp']<=str(end))].set_index('Timestamp')\n",
    "df_dockts_real_subset['Available Docks'].plot()\n",
    "base = df_dockts_real_subset['Available Docks'].iloc[0]\n",
    "(base + -1*df_ts_balance['California Ave & Milwaukee Ave'][:Num_days*day_row].cumsum()).plot(figsize=(16,9))\n",
    "plt.legend(['Acutal','agg_delta'])\n",
    "plt.title(f'{station_name} dock delta aggregated')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')\n",
    "a = plt.gca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167ce3f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "line_real = a.lines[0] # get the first line, there might be more\n",
    "line_agg_delta = a.lines[1]\n",
    "Real = line_real.get_ydata()\n",
    "Artificial = line_agg_delta.get_ydata()[:len(Real)]\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot( Real- Artificial)\n",
    "plt.title('difference between real and artificial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064fe32",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Num_days = 14\n",
    "start = datetime.strptime('2017-01-01','%Y-%m-%d')\n",
    "end = start + timedelta(days=Num_days)\n",
    "df_dockts_real_subset = df_dockts_real.loc[(df_dockts_real['Timestamp']>=str(start)) & (df_dockts_real['Timestamp']<=str(end))].set_index('Timestamp')\n",
    "df_dockts_real_subset['Available Docks'].plot()\n",
    "base = df_dockts_real_subset['Available Docks'].iloc[0]\n",
    "(base + -1*df_ts_balance['California Ave & Milwaukee Ave'][:Num_days*day_row].cumsum()).plot(figsize=(16,9))\n",
    "plt.legend(['Acutal','agg_delta'])\n",
    "plt.title(f'{station_name} dock delta aggregated')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')\n",
    "a = plt.gca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1566e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "line_real = a.lines[0] # get the first line, there might be more\n",
    "line_agg_delta = a.lines[1]\n",
    "Real = line_real.get_ydata()\n",
    "Artificial = line_agg_delta.get_ydata()[:len(Real)]\n",
    "plt.plot( Real- Artificial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44ce00",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Num_days = 30\n",
    "start = datetime.strptime('2017-01-01','%Y-%m-%d')\n",
    "end = start + timedelta(days=Num_days)\n",
    "df_dockts_real_subset = df_dockts_real.loc[(df_dockts_real['Timestamp']>=str(start)) & (df_dockts_real['Timestamp']<=str(end))].set_index('Timestamp')\n",
    "df_dockts_real_subset['Available Docks'].plot()\n",
    "base = df_dockts_real_subset['Available Docks'].iloc[0]\n",
    "(base + -1*df_ts_balance['California Ave & Milwaukee Ave'][:Num_days*day_row].cumsum()).plot(figsize=(16,9))\n",
    "plt.legend(['Acutal','agg_delta'])\n",
    "plt.title(f'{station_name} dock delta aggregated')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')\n",
    "###########\n",
    "a = plt.gca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51491ec0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "line_real = a.lines[0] # get the first line, there might be more\n",
    "line_agg_delta = a.lines[1]\n",
    "Real = line_real.get_ydata()\n",
    "Artificial = line_agg_delta.get_ydata()[:len(Real)]\n",
    "plt.plot( Real- Artificial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c93c33b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Verifying this is no coincidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d553df",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Num_days = 7\n",
    "start = datetime.strptime('2021-05-25','%Y-%m-%d')\n",
    "end = start + timedelta(days=Num_days)\n",
    "df_dockts_real_subset = df_dockts_real.loc[(df_dockts_real['Timestamp']>=str(start)) & (df_dockts_real['Timestamp']<=str(end))].set_index('Timestamp')\n",
    "df_dockts_real_subset['Available Docks'].plot()\n",
    "base = df_dockts_real_subset['Available Docks'].iloc[0]\n",
    "(base + -1*df_ts_balance['California Ave & Milwaukee Ave'][:Num_days*day_row].cumsum()).plot(figsize=(16,9))\n",
    "\n",
    "\n",
    "plt.title(f'{station_name} dock delta aggregated')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')\n",
    "a = plt.gca()\n",
    "#Cap\n",
    "plt.plot([15 for i in range(len(a.lines[0].get_ydata()))])\n",
    "plt.plot([-15 for i in range(len(a.lines[0].get_ydata()))])\n",
    "plt.legend(['Acutal','agg_delta','upper-bound','lower-bound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3cf6d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "line_real = a.lines[0] # get the first line, there might be more\n",
    "line_agg_delta = a.lines[1]\n",
    "Real = line_real.get_ydata()\n",
    "Artificial = line_agg_delta.get_ydata()[:len(Real)]\n",
    "plt.plot( Real- Artificial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8499ce9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### EDA Summary\n",
    "**Observation**:  \n",
    "- Taking Sample of Station (California Ave & Milwaukee Ave), I found that the Real TS and Artificial TS are very matching at first, and then diverges quickly. \n",
    "- There is also multiple small spkies along the way, but the difference plot demonstrates a step pattern   \n",
    "**Explanation**:  \n",
    "- Real data is capped by the total dock (15 in this station), whereas artificial data is not. \n",
    "- There are staff interventions, maneuver cars across stations \n",
    "- The artificial data only captures changes in 10 minute interval. Thus even ideally, it's only an estimate of the actual dock\n",
    "- Noise, it's possible that there are some random events happens lead to the inaccuracy of dock or travel history data   \n",
    "\n",
    "**Potential Metigation**:  \n",
    "- When cap is reached (-15,+15), and there are still natural growth/decline, this means staff intervention has already happened \n",
    "- Ignore the staff intervention, since it's completly controlable from the company's point of view \n",
    "- Predict the staff itervention   \n",
    "\n",
    "**Insights**:   \n",
    "- Conduct small time interval (5-60 minutes) forecasting to help users navigates \n",
    "- Conduct daily forecasting to help Divvy scheduling staff intervention schedules to minimize Dock overflow / short of bikes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.933265,
   "end_time": "2022-04-30T23:15:49.166192",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-30T23:15:33.232927",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
