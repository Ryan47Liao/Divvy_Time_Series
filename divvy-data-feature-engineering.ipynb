{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7588c7a8",
   "metadata": {
    "papermill": {
     "duration": 0.049244,
     "end_time": "2022-04-30T23:15:46.180545",
     "exception": false,
     "start_time": "2022-04-30T23:15:46.131301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Divvy Data (Feature) Engineering \n",
    "### Summary:\n",
    "This module transforms travel history data into time sereies data of the station dock capacity\n",
    "### Challenge & solutions\n",
    "1. Data formats are not consistent -> Manually transform the data \n",
    "2. There are ',' in csv file -> Manually transform the data \n",
    "3. Data size is relatively big -> Use big data platforms to perform the aggregation \n",
    "4. Station_ID is inconsistent across the years -> Use Station_Name as Primary key instead \n",
    "### Methodology\n",
    "1. Clean the data by aggregate into a single table \n",
    "2. Extract Day+Hour+10Min as Time_ID, group by Time_ID, Station_Name, aggregate by count\n",
    "3. Create placeholding dataframe df_main \n",
    "4. Join each station into the placeholding dataframe to create a pivoted timeseries table, where each row is a time snapshot, each column is the time series of each station \n",
    "5. Merge In & Out to form Capacity Change\n",
    "6. Finally, will compare the artificial delta agianst the actual station data and intepret the difference\n",
    "### Data\n",
    "- Source: https://www.divvybikes.com/system-data\n",
    "- Description: This dataset includes individual Divvy bike sharing trips, including the origin, destination, timestamps, and rider type for each trip.\n",
    "\n",
    "### Author: \n",
    "`Ryan Liao @2022/04/30 M.Sc Data Analytics Uchicago`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a870a8f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T23:15:46.277926Z",
     "iopub.status.busy": "2022-04-30T23:15:46.277538Z",
     "iopub.status.idle": "2022-04-30T23:15:47.687922Z",
     "shell.execute_reply": "2022-04-30T23:15:47.686399Z"
    },
    "papermill": {
     "duration": 1.462701,
     "end_time": "2022-04-30T23:15:47.691116",
     "exception": false,
     "start_time": "2022-04-30T23:15:46.228415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a1708",
   "metadata": {
    "papermill": {
     "duration": 0.047702,
     "end_time": "2022-04-30T23:15:47.787842",
     "exception": false,
     "start_time": "2022-04-30T23:15:47.740140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### 1. Clean the data by aggregate into a single table \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0f22edc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-30T23:15:47.885291Z",
     "iopub.status.busy": "2022-04-30T23:15:47.885010Z",
     "iopub.status.idle": "2022-04-30T23:15:47.988358Z",
     "shell.execute_reply": "2022-04-30T23:15:47.987389Z"
    },
    "papermill": {
     "duration": 0.153984,
     "end_time": "2022-04-30T23:15:47.990191",
     "exception": true,
     "start_time": "2022-04-30T23:15:47.836207",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bike-sharing-best-bike-share-programs.jpg',\n",
       " 'df_in_main_2019_2021.csv',\n",
       " 'df_out_main_2019_2021.csv',\n",
       " 'df_ts_balance_1719_ready.csv',\n",
       " 'df_ts_balance_1721.csv',\n",
       " 'df_ts_balance_1921.csv',\n",
       " 'df_ts_balance_1921_ready.csv',\n",
       " 'DivvyData_dock_hist1.csv',\n",
       " 'Dock_ts.csv',\n",
       " 'Dock_ts_flat.csv',\n",
       " 'station_dock_ts_in1719_ts_pivot.csv',\n",
       " 'station_dock_ts_out1719_ts_pivot.csv',\n",
       " 'ts_IN.csv',\n",
       " 'TS_in_1719_ts_pivot.csv',\n",
       " 'ts_OUT.csv',\n",
       " 'TS_OUT_1719_ts_pivot.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('E:\\\\Data\\\\divvy\\\\TimeS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb50c1f3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Divvy_Trips_2017_Q1.csv',\n",
       " 'Divvy_Trips_2017_Q2.csv',\n",
       " 'Divvy_Trips_2017_Q3.csv',\n",
       " 'Divvy_Trips_2017_Q4.csv',\n",
       " 'Divvy_Trips_2018_Q1.csv',\n",
       " 'Divvy_Trips_2018_Q2.csv',\n",
       " 'Divvy_Trips_2018_Q3.csv',\n",
       " 'Divvy_Trips_2018_Q4.csv',\n",
       " 'Divvy_Trips_2019_Q1.csv',\n",
       " 'Divvy_Trips_2019_Q2.csv',\n",
       " 'Divvy_Trips_2019_Q3.csv',\n",
       " 'Divvy_Trips_2019_Q4.csv',\n",
       " 'eve_attackers_ddl.sql',\n",
       " 'eve_encounters_ddl.sql',\n",
       " 'eve_items_ddl.sql',\n",
       " 'eve_wars_ddl.sql',\n",
       " 'README.txt',\n",
       " 'README_2017_q1q2.txt',\n",
       " '__MACOSX']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('E:\\\\Data\\\\divvy\\\\Travel_Hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7187af48",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Uniform the column names\n",
    "# DATABASE = {}\n",
    "# #unit_col = DATABASE['2017_Q1'].columns <- This depreciates\n",
    "# for path in os.listdir('E:\\\\Data\\\\divvy\\\\Travel_Hist'):\n",
    "#     file = \"E:\\\\Data\\\\divvy\\\\Travel_Hist\\\\\" + path \n",
    "#     if '.csv' in file:\n",
    "#         name = path.split('.')[0][-7:]\n",
    "#         df = pd.read_csv(file)\n",
    "#         df = df.rename({i:j for i,j in zip(df.columns,unit_col)},axis=1)\n",
    "#         df.start_time = df.start_time.apply(\n",
    "#             lambda x: datetime.strptime(x, \"%m/%d/%Y %H:%M:%S\")\n",
    "#         )\n",
    "#         df.end_time = df.end_time.apply(\n",
    "#                 lambda x: datetime.strptime(x, \"%m/%d/%Y %H:%M:%S\")\n",
    "#             )\n",
    "#         df.to_csv(file,index=False)\n",
    "#         print(name)\n",
    "#         #DATABASE[name] = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15e9ac27",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load into a single dataframe\n",
    "DATABASE = {} \n",
    "for path in os.listdir('E:\\\\Data\\\\divvy\\\\Travel_Hist'):\n",
    "    file = \"E:\\\\Data\\\\divvy\\\\Travel_Hist\\\\\" + path \n",
    "    if '.csv' in file:\n",
    "        name = path.split('.')[0][-7:]\n",
    "        df = pd.read_csv(file)\n",
    "        DATABASE[name] = pd.read_csv(file)\n",
    "        #print(name)\n",
    "        #print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b433d55f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "for db_name in DATABASE:\n",
    "    df = DATABASE[db_name]\n",
    "    df_all = pd.concat([df_all,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c940ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>bikeid</th>\n",
       "      <th>tripduration</th>\n",
       "      <th>from_station_id</th>\n",
       "      <th>from_station_name</th>\n",
       "      <th>to_station_id</th>\n",
       "      <th>to_station_name</th>\n",
       "      <th>usertype</th>\n",
       "      <th>gender</th>\n",
       "      <th>birthyear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13518905</td>\n",
       "      <td>2017-03-31 23:59:07</td>\n",
       "      <td>2017-04-01 00:13:24</td>\n",
       "      <td>5292</td>\n",
       "      <td>857</td>\n",
       "      <td>66</td>\n",
       "      <td>Clinton St &amp; Lake St</td>\n",
       "      <td>171</td>\n",
       "      <td>May St &amp; Cullerton St</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>1989.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13518904</td>\n",
       "      <td>2017-03-31 23:56:25</td>\n",
       "      <td>2017-04-01 00:00:21</td>\n",
       "      <td>4408</td>\n",
       "      <td>236</td>\n",
       "      <td>199</td>\n",
       "      <td>Wabash Ave &amp; Grand Ave</td>\n",
       "      <td>26</td>\n",
       "      <td>McClurg Ct &amp; Illinois St</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>1990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13518903</td>\n",
       "      <td>2017-03-31 23:55:33</td>\n",
       "      <td>2017-04-01 00:01:21</td>\n",
       "      <td>696</td>\n",
       "      <td>348</td>\n",
       "      <td>520</td>\n",
       "      <td>Greenview Ave &amp; Jarvis Ave</td>\n",
       "      <td>432</td>\n",
       "      <td>Clark St &amp; Lunt Ave</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Female</td>\n",
       "      <td>1979.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13518902</td>\n",
       "      <td>2017-03-31 23:54:46</td>\n",
       "      <td>2017-03-31 23:59:34</td>\n",
       "      <td>4915</td>\n",
       "      <td>288</td>\n",
       "      <td>110</td>\n",
       "      <td>Dearborn St &amp; Erie St</td>\n",
       "      <td>142</td>\n",
       "      <td>McClurg Ct &amp; Erie St</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>1985.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13518901</td>\n",
       "      <td>2017-03-31 23:53:33</td>\n",
       "      <td>2017-04-01 00:00:28</td>\n",
       "      <td>4247</td>\n",
       "      <td>415</td>\n",
       "      <td>327</td>\n",
       "      <td>Sheffield Ave &amp; Webster Ave</td>\n",
       "      <td>331</td>\n",
       "      <td>Halsted St &amp; Blackhawk St (*)</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Female</td>\n",
       "      <td>1989.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trip_id           start_time             end_time  bikeid tripduration  \\\n",
       "0  13518905  2017-03-31 23:59:07  2017-04-01 00:13:24    5292          857   \n",
       "1  13518904  2017-03-31 23:56:25  2017-04-01 00:00:21    4408          236   \n",
       "2  13518903  2017-03-31 23:55:33  2017-04-01 00:01:21     696          348   \n",
       "3  13518902  2017-03-31 23:54:46  2017-03-31 23:59:34    4915          288   \n",
       "4  13518901  2017-03-31 23:53:33  2017-04-01 00:00:28    4247          415   \n",
       "\n",
       "   from_station_id            from_station_name  to_station_id  \\\n",
       "0               66         Clinton St & Lake St            171   \n",
       "1              199       Wabash Ave & Grand Ave             26   \n",
       "2              520   Greenview Ave & Jarvis Ave            432   \n",
       "3              110        Dearborn St & Erie St            142   \n",
       "4              327  Sheffield Ave & Webster Ave            331   \n",
       "\n",
       "                 to_station_name    usertype  gender  birthyear  \n",
       "0          May St & Cullerton St  Subscriber    Male     1989.0  \n",
       "1       McClurg Ct & Illinois St  Subscriber    Male     1990.0  \n",
       "2            Clark St & Lunt Ave  Subscriber  Female     1979.0  \n",
       "3           McClurg Ct & Erie St  Subscriber    Male     1985.0  \n",
       "4  Halsted St & Blackhawk St (*)  Subscriber  Female     1989.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9b2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_all.groupby(['to_station_id',\t'to_station_name']).trip_id.count().to_csv('E:\\Data\\divvy/1719station_id_name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a635b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Examine column types to identify areas that requiring transformation\n",
    "# start_time,end_time      -> Need to be mapped into datetime objects for easier manipulation\n",
    "# tripduration             -> Supposed to be float but instead containing werid formatted string, such as \"1,102\"\n",
    "df_all.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d447b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.start_time = df_all.start_time.apply(\n",
    "    lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    ")\n",
    "df_all.end_time = df_all.end_time.apply(\n",
    "        lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86f8e3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.start_time.head().apply(lambda x:str(x)[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4dfa00",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.tripduration = df_all.tripduration.apply(lambda x:float(str(x).replace(',','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31809104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('E:\\Data\\divvy/Divvy_Trips_1719.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea5e45",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 2. Extract Day+Hour+10Min as Time_ID, group by Time_ID, Station_Name, aggregate by count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14564a6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DH10M_start = df_all.start_time.apply(lambda x:str(x)[:-4])\n",
    "DH10M_end = df_all.end_time.apply(lambda x:str(x)[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ab971",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all['DH10M_start'] = DH10M_start\n",
    "df_all['DH10M_end'] = DH10M_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c6a725",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Already Done\n",
    "ts_OUT =  pd.DataFrame(df_all.groupby(by=['DH10M_start','from_station_name']).trip_id.count())\n",
    "ts_IN =   pd.DataFrame(df_all.groupby(by=['DH10M_end','to_station_name']).trip_id.count())\n",
    "ts_IN = ts_IN.rename({'trip_id':'count_enter'},axis=1)\n",
    "ts_OUT = ts_OUT.rename({'trip_id':'count_leave'},axis=1)\n",
    "ts_OUT.DH10M_start = ts_OUT.DH10M_start.apply(lambda x: datetime.strptime(x+'0:00', \"%Y-%m-%d %H:%M:%S\"))\n",
    "ts_OUT = ts_OUT.set_index('DH10M_start',drop=True)\n",
    "ts_IN.DH10M_end = ts_IN.DH10M_end.apply(lambda x: datetime.strptime(x+'0:00', \"%Y-%m-%d %H:%M:%S\"))\n",
    "ts_IN = ts_IN.set_index('DH10M_end',drop=True)\n",
    "ts_IN.to_csv('E:\\\\Data\\\\divvy\\\\ts_IN.csv')\n",
    "ts_OUT.to_csv('E:\\\\Data\\\\divvy\\\\ts_OUT.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a9919",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ts_IN = pd.read_csv('E:\\\\Data\\\\divvy\\\\ts_IN.csv')\n",
    "ts_OUT = pd.read_csv('E:\\\\Data\\\\divvy\\\\ts_OUT.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999dc3e6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 3. Create placeholding dataframe df_main \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c1f60",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TS_gen(time_start,time_end,time_unit = timedelta(minutes=10)):\n",
    "    \"\"\"\n",
    "    __summary__\n",
    "    create a dataframe rangeing from time_start -> time_end\n",
    "    \"\"\"\n",
    "    out_dict = {'time_stamp':[]}\n",
    "    current_time = time_start\n",
    "    while current_time < time_end:\n",
    "        out_dict['time_stamp'].append(current_time)\n",
    "        current_time += time_unit\n",
    "    out = pd.DataFrame(out_dict)\n",
    "    out.time_stamp = out.time_stamp.apply(lambda x: str(x))\n",
    "    return out.set_index('time_stamp',drop=True)\n",
    "    \n",
    "def FillSpace(df,tablename,time_start,time_end,station_names,\n",
    "              station_col_name,count_name,time_stamp_name,file_path):\n",
    "    \"\"\"\n",
    "    __summary__\n",
    "    Create a pivoted time series dataframe\n",
    "    \"\"\"\n",
    "    ## Depreciated in newer version\n",
    "    # df['time']=df[f'{tablename}.{prefix}_at_time'].apply(lambda x: f' {x}0:00')\n",
    "    # df['start_timestamp'] = df[f'{tablename}.{prefix}_at_day'] + df['time']\n",
    "    # df['start_timestamp'] = df['start_timestamp'].apply(\n",
    "    #             lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "    #         )\n",
    "    print('data prepared')\n",
    "    ########## Joining the tables ###########\n",
    "    df_in_main = TS_gen(time_start,time_end)\n",
    "    print('df_main generated,joining now...')\n",
    "    count = 0 \n",
    "    total = len(station_names)\n",
    "    for station_name in tqdm(station_names):\n",
    "        count+=1\n",
    "        print(f'{station_name}|{count}/{total}')\n",
    "        df_sub_set = df[df[f'{station_col_name}'] == station_name]\n",
    "        df_sub_set = df_sub_set.set_index(time_stamp_name,drop=True)\n",
    "        df_in_main = df_in_main.join(df_sub_set[count_name]).fillna(0).rename({count_name:station_name},axis=1)\n",
    "    #Finally\n",
    "    df_in_main.to_csv(f'{file_path}\\\\{tablename}_ts_pivot.csv')\n",
    "    return df_in_main\n",
    "##########\n",
    "time_start = datetime(year =2017,month=1,day=1,hour=0,minute=20)\n",
    "time_end = datetime(year =2019,month=9,day=30,hour=23,minute=50)\n",
    "station_names = set(ts_IN.to_station_name.unique()).union(set(ts_OUT.from_station_name.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c990ea",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_in = FillSpace(ts_IN,\"TS_in_1719\",time_start,time_end,station_names,\n",
    "                     'to_station_name','count_enter','DH10M_end','E:\\\\Data\\\\divvy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18658690",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_out = FillSpace(ts_OUT,\"TS_OUT_1719\",time_start,time_end,station_names,\n",
    "          'from_station_name','count_leave','DH10M_start','E:\\\\Data\\\\divvy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f16445",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_out = pd.read_csv('E:\\\\Data\\\\divvy\\\\TS_OUT_1719_ts_pivot.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6dd80",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 4. Join each station into the placeholding dataframe to create a pivoted timeseries table, where each row is a time snapshot, each column is the time series of each station "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e37f11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### 5. Merge In & Out to form Capacity Change; Visualize time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55228630",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_out['Damen Ave & Pierce Ave'].plot()\n",
    "df_ts_in['Damen Ave & Pierce Ave'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72250047",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_balance = df_ts_in - df_ts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045e4c0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_balance['Damen Ave & Pierce Ave'].plot(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c69c972",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_balance['Damen Ave & Pierce Ave'][:1000].plot(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc69270",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_balance.to_csv('E:\\\\Data\\\\divvy\\\\df_ts_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddc8c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This plots aggregated delta (assuming no staff manuvouring)\n",
    "hour_row = 6 \n",
    "day_row = 24 * hour_row\n",
    "station_name = df_ts_in.columns[0:5]#'Lake Park Ave & 47th St'\n",
    "df_ts_balance[station_name][:60*day_row].cumsum().plot(figsize=(16,9))\n",
    "plt.title(f'{station_name} dock delta aggregated')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')\n",
    "# df_ts_in[station_name].cumsum().plot(figsize=(16,9))\n",
    "# df_ts_out[station_name].cumsum().plot(figsize=(16,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87523ba3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This plots rolling cumsum, resetting each day. \n",
    "df_ts_balance = df_ts_balance.reset_index()\n",
    "df_ts_balance.insert( 0, 'date' ,df_ts_balance.time_stamp.apply(lambda x:x.split(' ')[0]))\n",
    "df_ts_balance = df_ts_balance.set_index('time_stamp',drop=True)\n",
    "df_ts_balance_CS = df_ts_balance.groupby('date').cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757e304",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plotting\n",
    "hour_row = 6 \n",
    "day_row = 24 * hour_row\n",
    "station_name = df_ts_in.columns[0:5]#'Lake Park Ave & 47th St'\n",
    "df_ts_balance_CS[station_name][:30*day_row].plot(figsize=(16,9))\n",
    "plt.title(f'{station_name}  dock delta daily reset')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29172524",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Compare against actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662518c2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "station_name = 'California Ave & Milwaukee Ave'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0fd3f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_balance[station_name][:60*day_row].cumsum().plot(figsize=(16,9))\n",
    "plt.title(f'{station_name} dock delta aggregated')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870bce72",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_ts_balance_CS[station_name][:30*day_row].plot(figsize=(16,9))\n",
    "plt.title(f'{station_name}  dock delta daily reset')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a347da2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dockts_real = pd.read_csv('E:\\\\Data\\\\divvy\\\\TimeS\\\\DivvyData_dock_hist1.csv')\n",
    "df_dockts_real.Timestamp = df_dockts_real.Timestamp.apply(\n",
    "            lambda x: str(datetime.strptime(x, \"%m/%d/%Y %H:%M:%S %p\"))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be29f077",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dockts_real = df_dockts_real.rename({'Total Docks':'Total_Docks'},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4fdee",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Num_days = 7\n",
    "start = datetime.strptime('2017-01-01','%Y-%m-%d')\n",
    "end = start + timedelta(days=Num_days)\n",
    "df_dockts_real_subset = df_dockts_real.loc[(df_dockts_real['Timestamp']>=str(start)) & (df_dockts_real['Timestamp']<=str(end))].set_index('Timestamp')\n",
    "df_dockts_real_subset['Available Docks'].plot()\n",
    "base = df_dockts_real_subset['Available Docks'].iloc[0]\n",
    "(base + -1*df_ts_balance['California Ave & Milwaukee Ave'][:Num_days*day_row].cumsum()).plot(figsize=(16,9))\n",
    "plt.legend(['Acutal','agg_delta'])\n",
    "plt.title(f'{station_name} dock delta aggregated')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')\n",
    "a = plt.gca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167ce3f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "line_real = a.lines[0] # get the first line, there might be more\n",
    "line_agg_delta = a.lines[1]\n",
    "Real = line_real.get_ydata()\n",
    "Artificial = line_agg_delta.get_ydata()[:len(Real)]\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot( Real- Artificial)\n",
    "plt.title('difference between real and artificial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064fe32",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Num_days = 14\n",
    "start = datetime.strptime('2017-01-01','%Y-%m-%d')\n",
    "end = start + timedelta(days=Num_days)\n",
    "df_dockts_real_subset = df_dockts_real.loc[(df_dockts_real['Timestamp']>=str(start)) & (df_dockts_real['Timestamp']<=str(end))].set_index('Timestamp')\n",
    "df_dockts_real_subset['Available Docks'].plot()\n",
    "base = df_dockts_real_subset['Available Docks'].iloc[0]\n",
    "(base + -1*df_ts_balance['California Ave & Milwaukee Ave'][:Num_days*day_row].cumsum()).plot(figsize=(16,9))\n",
    "plt.legend(['Acutal','agg_delta'])\n",
    "plt.title(f'{station_name} dock delta aggregated')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')\n",
    "a = plt.gca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d1566e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "line_real = a.lines[0] # get the first line, there might be more\n",
    "line_agg_delta = a.lines[1]\n",
    "Real = line_real.get_ydata()\n",
    "Artificial = line_agg_delta.get_ydata()[:len(Real)]\n",
    "plt.plot( Real- Artificial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44ce00",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Num_days = 30\n",
    "start = datetime.strptime('2017-01-01','%Y-%m-%d')\n",
    "end = start + timedelta(days=Num_days)\n",
    "df_dockts_real_subset = df_dockts_real.loc[(df_dockts_real['Timestamp']>=str(start)) & (df_dockts_real['Timestamp']<=str(end))].set_index('Timestamp')\n",
    "df_dockts_real_subset['Available Docks'].plot()\n",
    "base = df_dockts_real_subset['Available Docks'].iloc[0]\n",
    "(base + -1*df_ts_balance['California Ave & Milwaukee Ave'][:Num_days*day_row].cumsum()).plot(figsize=(16,9))\n",
    "plt.legend(['Acutal','agg_delta'])\n",
    "plt.title(f'{station_name} dock delta aggregated')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')\n",
    "###########\n",
    "a = plt.gca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51491ec0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "line_real = a.lines[0] # get the first line, there might be more\n",
    "line_agg_delta = a.lines[1]\n",
    "Real = line_real.get_ydata()\n",
    "Artificial = line_agg_delta.get_ydata()[:len(Real)]\n",
    "plt.plot( Real- Artificial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c93c33b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Verifying this is no coincidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d553df",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Num_days = 7\n",
    "start = datetime.strptime('2021-05-25','%Y-%m-%d')\n",
    "end = start + timedelta(days=Num_days)\n",
    "df_dockts_real_subset = df_dockts_real.loc[(df_dockts_real['Timestamp']>=str(start)) & (df_dockts_real['Timestamp']<=str(end))].set_index('Timestamp')\n",
    "df_dockts_real_subset['Available Docks'].plot()\n",
    "base = df_dockts_real_subset['Available Docks'].iloc[0]\n",
    "(base + -1*df_ts_balance['California Ave & Milwaukee Ave'][:Num_days*day_row].cumsum()).plot(figsize=(16,9))\n",
    "\n",
    "\n",
    "plt.title(f'{station_name} dock delta aggregated')\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('dock delta')\n",
    "a = plt.gca()\n",
    "#Cap\n",
    "plt.plot([15 for i in range(len(a.lines[0].get_ydata()))])\n",
    "plt.plot([-15 for i in range(len(a.lines[0].get_ydata()))])\n",
    "plt.legend(['Acutal','agg_delta','upper-bound','lower-bound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab3cf6d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "line_real = a.lines[0] # get the first line, there might be more\n",
    "line_agg_delta = a.lines[1]\n",
    "Real = line_real.get_ydata()\n",
    "Artificial = line_agg_delta.get_ydata()[:len(Real)]\n",
    "plt.plot( Real- Artificial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8499ce9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### EDA Summary\n",
    "**Observation**:  \n",
    "- Taking Sample of Station (California Ave & Milwaukee Ave), I found that the Real TS and Artificial TS are very matching at first, and then diverges quickly. \n",
    "- There is also multiple small spkies along the way, but the difference plot demonstrates a step pattern   \n",
    "**Explanation**:  \n",
    "- Real data is capped by the total dock (15 in this station), whereas artificial data is not. \n",
    "- There are staff interventions, maneuver cars across stations \n",
    "- The artificial data only captures changes in 10 minute interval. Thus even ideally, it's only an estimate of the actual dock\n",
    "- Noise, it's possible that there are some random events happens lead to the inaccuracy of dock or travel history data   \n",
    "\n",
    "**Potential Metigation**:  \n",
    "- When cap is reached (-15,+15), and there are still natural growth/decline, this means staff intervention has already happened \n",
    "- Ignore the staff intervention, since it's completly controlable from the company's point of view \n",
    "- Predict the staff itervention   \n",
    "\n",
    "**Insights**:   \n",
    "- Conduct small time interval (5-60 minutes) forecasting to help users navigates \n",
    "- Conduct daily forecasting to help Divvy scheduling staff intervention schedules to minimize Dock overflow / short of bikes\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01db3f9c0a93da5e24049b5a0a9f3aab8ac93704067afbc402ff1a6ee3910303"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('msca')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.933265,
   "end_time": "2022-04-30T23:15:49.166192",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-30T23:15:33.232927",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
